{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9299b8c1-7950-4e4d-a0d7-353a2b6be92a",
   "metadata": {},
   "source": [
    "# ctscan_mobilenetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0287c3-a650-4613-8447-bdff8016696d",
   "metadata": {},
   "source": [
    "Source: https://www.kaggle.com/code/ahmedtronic/covid-19-binary-classification-densenet169-98"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1646c16-8e67-4a24-a6f0-795e47f5aedc",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabf6168-3a3a-48e8-8224-1ee448b27b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import v2\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torchvision import models, datasets\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1bfcce-eb52-4774-8651-74fbb82ed817",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ba285f-750c-4f8a-822a-1093aa80b7e1",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e9e2ab-86ab-4fe4-89bf-7d0d131ae9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_curves_tuning(train_loss, valid_loss, train_acc, valid_acc,\n",
    "                           fine_tune_epoch=None):\n",
    "    \n",
    "    # Get training and validation data from initial training\n",
    "    tacc = train_acc\n",
    "    tloss = train_loss\n",
    "    vacc = valid_acc\n",
    "    vloss = valid_loss\n",
    "\n",
    "    total_epochs = [i+1 for i in range(len(tacc))]\n",
    "\n",
    "    # Find best epoch based on validation loss and accuracy\n",
    "    index_loss = np.argmin(vloss)  # epoch with the lowest validation loss\n",
    "    val_lowest = vloss[index_loss]\n",
    "    \n",
    "    index_acc = np.argmax(vacc)  # epoch with the highest validation accuracy\n",
    "    acc_highest = vacc[index_acc]\n",
    "\n",
    "    # Define plot labels\n",
    "    sc_label = 'best epoch= ' + str(index_loss + 1)\n",
    "    vc_label = 'best epoch= ' + str(index_acc + 1)\n",
    "\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8))\n",
    "\n",
    "    # Plot loss curves\n",
    "    axes[0].plot(total_epochs, tloss, 'r', label='Training Loss')\n",
    "    axes[0].plot(total_epochs, vloss, 'g', label='Validation Loss')\n",
    "    axes[0].scatter(index_loss + 1, val_lowest, s=150, c='blue', label=sc_label)\n",
    "\n",
    "    # Add fine-tuning marker\n",
    "    if fine_tune_epoch:\n",
    "        axes[0].axvline(x=fine_tune_epoch, color='orange', linestyle='--',\n",
    "                        label='Start Fine Tuning')\n",
    "\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].set_xlabel('Epochs')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Plot accuracy curves\n",
    "    axes[1].plot(total_epochs, tacc, 'r', label='Training Accuracy')\n",
    "    axes[1].plot(total_epochs, vacc, 'g', label='Validation Accuracy')\n",
    "    axes[1].scatter(index_acc + 1, acc_highest, s=150, c='blue', label=vc_label)\n",
    "\n",
    "    # Add fine-tuning marker\n",
    "    if fine_tune_epoch:\n",
    "        axes[1].axvline(x=fine_tune_epoch, color='orange', linestyle='--',\n",
    "                        label='Start Fine Tuning')\n",
    "\n",
    "    axes[1].set_title('Training and Validation Accuracy')\n",
    "    axes[1].set_xlabel('Epochs')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3267f8e0-5a99-4aa8-b428-cd3a6a9b63b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    total_correct = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for batch_number, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #forward path\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "                \n",
    "        #bacward path\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_number and batch_number % 50 == 0):\n",
    "            current_loss = epoch_loss / (batch_number + 1)\n",
    "            current_acc = total_correct / total_predictions\n",
    "            print(f\"Batch [{batch_number}/{len(train_loader)}], Loss:{current_loss:0.6f}, Accuracy: {current_acc*100:.3f}%\")\n",
    "            \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    avg_acc = total_correct / total_predictions\n",
    "    \n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c0984d-1b72-419d-af4a-fdd27ec957f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val(model):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    total_correct = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    # no gradient cal for tensor, same for: requires_grad = False and detach()\n",
    "        for batch_num, (images, labels) in enumerate(valid_loader):\n",
    "    \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            #forward path\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "            \n",
    "    avg_loss = epoch_loss / len(valid_loader)\n",
    "    avg_acc = total_correct / total_predictions\n",
    "\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5056cde-77ba-4962-8f55-e56cf8b89a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test(model):\n",
    "    \"\"\"\n",
    "    Calculates the test loss and accuracy for the given model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained model for evaluation.\n",
    "    \n",
    "    Returns:\n",
    "    - test_loss: Average test loss over the test dataset.\n",
    "    - test_accuracy: Test accuracy over the test dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    total_correct = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_num, (images, labels) in enumerate(test_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            output = model(images)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            loss = criterion(output, labels)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "    # Average test loss\n",
    "    avg_loss = epoch_loss / len(test_loader)\n",
    "    avg_acc = total_correct / total_predictions\n",
    "    \n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c847c7bb-6d8d-4851-a3c1-402c4084e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predict(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Get all predictions and true labels from the data loader.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained model for predictions.\n",
    "    - data_loader: DataLoader for the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - y_true: List of true labels.\n",
    "    - y_pred: List of predicted labels.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_num, (images, labels) in enumerate(data_loader):\n",
    "            \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "    return true_labels, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26142a8-8381-442a-b6cf-ce8b6e5219ac",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc86c7b-9251-4521-ad5b-04ea4562ec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"datasets/ctscan/raw\"\n",
    "covid_dir = \"datasets/ctscan/raw/COVID\"\n",
    "non_covid_dir = \"datasets/ctscan/raw/non-COVID\"\n",
    "\n",
    "image_size = (224, 224)\n",
    "dataset = datasets.ImageFolder(root=dataset_dir)\n",
    "class_names = dataset.classes\n",
    "num_classes = len(class_names)\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a7004c-cb10-440f-b13d-ca993d46b187",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_paths = glob.glob(f\"{dataset_dir}/**/*.png\")\n",
    "covid_paths = glob.glob(f\"{covid_dir}/*.png\")\n",
    "non_covid_paths = glob.glob(f\"{non_covid_dir}/*.png\")\n",
    "\n",
    "print(\"Total:\", len(dataset_paths))\n",
    "print(\"Covid:\", len(covid_paths))\n",
    "print(\"Non-Covid:\", len(non_covid_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001f7b85-dd33-4eee-b911-0f7011aa9219",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths, test_paths = train_test_split(dataset_paths, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdb4395-23e2-47d4-89e4-63f5b049f9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_paths, test_paths = train_test_split(test_paths, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8d2ca2-7e0b-41f6-b480-d0f4666b8074",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train:\", len(train_paths))\n",
    "print(\"Val:\", len(val_paths))\n",
    "print(\"Test:\", len(test_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b92aa9-bab2-45b2-a3e9-9823e9d7f279",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_path = os.path.join(\"COVID\", \"Covid\")\n",
    "\n",
    "# Count the number of covid and non_covid in the validation set\n",
    "train_covid_count = len([path for path in train_paths if covid_path in path])\n",
    "train_non_covid_count = len([path for path in train_paths if \"Non-Covid\" in path])\n",
    "\n",
    "# Count the number of covid and non_covid in the validation set\n",
    "valid_covid_count = len([path for path in val_paths if covid_path in path])\n",
    "valid_non_covid_count = len([path for path in val_paths if \"Non-Covid\" in path])\n",
    "\n",
    "# Count the number of covid and non_covid in the test set\n",
    "test_covid_count = len([path for path in test_paths if covid_path in path])\n",
    "test_non_covid_count = len([path for path in test_paths if \"Non-Covid\" in path])\n",
    "\n",
    "# Print the counts\n",
    "print(\"Train set - Covid:\", train_covid_count)\n",
    "print(\"Train set - Non-Covid:\", train_non_covid_count)\n",
    "print(\"Validation set - Covid:\", valid_covid_count)\n",
    "print(\"Validation set - Non-Covid:\", valid_non_covid_count)\n",
    "print(\"Test set - Covid:\", test_covid_count)\n",
    "print(\"Test set - Non-Covid:\", test_non_covid_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d2a29-2916-4198-b9d1-64b8e127b7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "random.shuffle(train_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26dd612-2236-4a1a-a3f7-fc410a339570",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(covid_paths[0])\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a00a78-a873-4045-aea1-b4d61381ea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(non_covid_paths[0])\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da33f58-347d-4569-b914-d56cccb66d9a",
   "metadata": {},
   "source": [
    "## Data Augmentation & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096f8795-dd11-488e-815a-58d75aa8e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = v2.Compose([\n",
    "    v2.Resize(image_size),\n",
    "    v2.PILToTensor(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.RandomHorizontalFlip(p = 0.5), # 50 % from images will apply to\n",
    "    v2.RandomVerticalFlip(p = 0.5), # 50 % from images will apply to\n",
    "    v2.RandomRotation(10),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "valid_transform = v2.Compose([\n",
    "    v2.Resize(image_size),\n",
    "    v2.PILToTensor(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform = {\n",
    "    \"train_transform\":train_transform,\n",
    "    \"valid_transform\":valid_transform\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936edafe-0d3b-432e-8877-a12a2d8144ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovidDataset(Dataset):\n",
    "    def __init__(self, paths, transform = None, is_train =True):\n",
    "        # data loadig\n",
    "        self.paths = paths\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.paths[index])\n",
    "        img = img.convert(\"RGB\") # Some images 4 channels\n",
    "        label = self.paths[index][-15:-10]\n",
    "\n",
    "        if self.transform:\n",
    "            if self.is_train:\n",
    "                img = self.transform[\"train_transform\"](img)\n",
    "            else:\n",
    "                img = self.transform[\"valid_transform\"](img)\n",
    "        \n",
    "        return img, (1 if label == \"Covid\" else 0)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5e3c0f-738c-43c8-91dc-5ec105e6eba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CovidDataset(train_paths, transform)\n",
    "valid_data = CovidDataset(val_paths, transform, is_train=False)\n",
    "test_data = CovidDataset(test_paths, transform, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def44a01-bc52-4a11-80dd-54711154e3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=16)\n",
    "valid_loader = DataLoader(valid_data, shuffle=False, batch_size=16)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a7b7cc-c8cf-48a3-9176-2d76f9bcc54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader))[0].shape # images must have the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5f40c0-dd36-41fb-ae72-8828f5a59ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2d2d82-fa99-4a05-a9ca-0766c3a2b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = train_data[1][0]\n",
    "plt.grid(False)\n",
    "plt.axis(False)\n",
    "plt.title(\"Covid\" if train_data[1][1] == 1 else \"Non-Covid\")\n",
    "plt.imshow(img.permute(1,2,0)); # img in tensor[channels, h, w]\n",
    "# matplolib needs it [h, w, channels] so we use permute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a65e7f-f9b1-4c51-9ebf-fe9d2ee0194d",
   "metadata": {},
   "source": [
    "## MobileNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f0992f-5c4e-4252-bb84-18619f93b994",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209dcfcf-2f1f-46af-b657-2592d25314ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.mobilenet_v2(weights='IMAGENET1K_V1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262be3cf-12bc-4c8b-a787-ce93474047c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in net.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "net.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cba79f0-7e2e-4ed8-8b9c-8a6d9e605a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.last_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa6a4e7-831d-4388-950f-0b699eef45b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.classifier = nn.Sequential(\n",
    "    nn.Linear(net.last_channel, 512),  # Example: Add a fully connected layer\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(512, num_classes)  # Final output layer for binary classification (COVID vs. non-COVID)\n",
    ")\n",
    "\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a31e6cb-fe2f-4d68-9fe2-089a7c37785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping parameters\n",
    "best_score = float(\"inf\") # ModelCheckpoint\n",
    "patience = 0 # EarlyStopping\n",
    "\n",
    "initial_epochs = 20  # Number of epochs before fine-tuning\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.classifier.parameters(), lr=1e-3, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251a42eb-2104-4402-9925-76866461bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "valid_losses = []\n",
    "train_accs = []\n",
    "valid_accs = []\n",
    "\n",
    "# train loop\n",
    "\n",
    "for epoch in range(initial_epochs):\n",
    "    print(f\"Epoch[{epoch+1}/{initial_epochs}],    \", end=\"\")\n",
    "\n",
    "    avg_train_loss, train_acc = train_epoch(net)\n",
    "    avg_valid_loss, valid_acc = get_val(net)\n",
    "\n",
    "    # ModelCheckpoint & Save best model \n",
    "    if (avg_valid_loss < best_score):\n",
    "        best_score = avg_valid_loss\n",
    "        torch.save(net.state_dict(), f\"model_net.pth\")\n",
    "        patience = 0\n",
    "        \n",
    "    # EarlyStopping\n",
    "    else:\n",
    "        patience += 1\n",
    "        if (patience >= 10):\n",
    "            print(f\"Early stopping triggered after {epoch} epochs during training.\")\n",
    "            break\n",
    "        \n",
    "    train_losses.append(avg_train_loss)\n",
    "    valid_losses.append(avg_valid_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_accs.append(valid_acc)\n",
    "\n",
    "    print(f'Train Loss = {avg_train_loss:.6f} -- Train Accuracy = {train_acc*100:2.3f}%')\n",
    "    print(f'Validation Loss = {avg_valid_loss:.6f} -- Validation Accuracy = {valid_acc*100:2.3f}%')\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d349f55-24ad-4281-8d92-66be84753e76",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978524ab-501a-4a96-8465-dfc4a2247a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e309e4b-1fdc-4c21-91d2-c0732c039409",
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in net.features[-6:].parameters():\n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515f9b90-5e6a-40d8-a78b-f2b2936a1fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower learning rate for fine-tuning\n",
    "optimizer = optim.Adam(net.parameters(), lr=3e-5, weight_decay=0.01)\n",
    "\n",
    "# Early stopping parameters\n",
    "best_score = float(\"inf\") # ModelCheckpoint\n",
    "patience = 0 # EarlyStopping\n",
    "fine_tuning_epochs = 50  # number of fine-tuning epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760a9a79-1a3c-4c3b-8866-c6841a2ba5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop\n",
    "\n",
    "for epoch in range(fine_tuning_epochs):\n",
    "    print(f\"Epoch[{epoch+1}/{fine_tuning_epochs}],    \", end=\"\")\n",
    "\n",
    "    avg_train_loss, train_acc = train_epoch(net)\n",
    "    avg_valid_loss, valid_acc = get_val(net)\n",
    "\n",
    "\n",
    "    # ModelCheckpoint & Save best model \n",
    "    if (avg_valid_loss < best_score):\n",
    "        best_score = avg_valid_loss\n",
    "        torch.save(net.state_dict(), f\"model_net_finetuned.pth\")\n",
    "        patience = 0\n",
    "        \n",
    "    # EarlyStopping Callback\n",
    "    else:\n",
    "        patience += 1\n",
    "        if (patience >= 10):\n",
    "            print(f\"Early stopping triggered after {epoch} epochs during training.\")\n",
    "            break\n",
    "    \n",
    "        \n",
    "    train_losses.append(avg_train_loss)\n",
    "    valid_losses.append(avg_valid_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_accs.append(valid_acc)\n",
    "\n",
    "    print(f'Train Loss = {avg_train_loss:.6f} -- Train Accuracy = {train_acc*100:2.3f}%')\n",
    "    print(f'Validation Loss = {avg_valid_loss:.6f} -- Validation Accuracy = {valid_acc*100:2.3f}%')\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6785dc8-92a4-4267-b027-13e19669a267",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curves_tuning(train_losses, valid_losses, train_accs, valid_accs, fine_tune_epoch=initial_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998790d6-e59d-48df-816f-5d44562a0c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_loss, validation_accuracy = get_val(net)\n",
    "print(f'Validation Loss: {validation_loss:.4f}, Validation Accuracy: {validation_accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9954de-ecba-4e2e-be67-76d22711b5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = get_test(net)\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02fffed-a248-4c58-9419-6530e321ae20",
   "metadata": {},
   "source": [
    "## Classification Report & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fd1f2c-e4dc-45ee-9403-4b52c5e77435",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = get_predict(net, test_loader, device)\n",
    "print(classification_report(y_true, y_pred, target_names= class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c83dcc-6f61-4816-957e-c1ed7a958637",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plot_confusion_matrix(cm, class_names=class_names, figsize=(8,6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d92213-2639-4703-846a-ca72ac63728d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
